{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgg1FRTSW3dc"
      },
      "source": [
        "# ETL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7NJ0YPeYDGP"
      },
      "source": [
        "## Enviroment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzHO6LfemJU2"
      },
      "source": [
        "To keep constant track of the files used in this Notebook, I will use my own Google Drive to store the datasets and models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob7UqbkxoFo-",
        "outputId": "9e25797d-2a4e-4e11-afb7-58cc9310dbba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRb2QYqXmMnP"
      },
      "source": [
        "In addition, to enable other people to run this Notebook, I defined a dictionary of paths to maintain consistency throughout the Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVzrrErGoAhN"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "\n",
        "BASE_PATH = \"/content/drive/MyDrive/TEC/AI/ProyectoBenji\"\n",
        "\n",
        "PATHS = {\n",
        "  \"RAW_DATA_PATH\": f\"{BASE_PATH}/data/raw\",\n",
        "  \"PROCESSED_DATA_PATH\": f\"{BASE_PATH}/data/processed\",\n",
        "  \"CLEAN_DATA_PATH\": f\"{BASE_PATH}/data/clean\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5bgOQ8cpWmZX"
      },
      "outputs": [],
      "source": [
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Natural Language Toolkit\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Kaggle\n",
        "import kagglehub\n",
        "\n",
        "# Utilities\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "\n",
        "# PyArrow for efficient Parquet writing\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTdrxZmA4tbV"
      },
      "source": [
        "## Data collection & exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbJgAr6tMQ2C"
      },
      "source": [
        "The Yelp Academic Dataset comprises 6,990,280 reviews distributed across multiple JSON files, totaling 8.65 GB. For this analysis, I focus exclusively on the yelp_academic_dataset_review.json file (5.09 GB), which contains the core review data including text content, star ratings, timestamps, and user engagement metrics.\n",
        "\n",
        "This file provides all necessary features for sentiment analysis, eliminating the need for additional business or user metadata files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I9fcOQMXn7ZG"
      },
      "outputs": [],
      "source": [
        "#@title YelpDataLoader\n",
        "\n",
        "class YelpDataLoader:\n",
        "  def __init__(self):\n",
        "    self.kaggle_id = \"yelp-dataset/yelp-dataset\"\n",
        "    self.kaggle_path = None\n",
        "\n",
        "  def _load_dataframe_if_exists(self, output_path):\n",
        "    if os.path.exists(output_path):\n",
        "      print(f\"\\nAlready exists: {output_path}\")\n",
        "      df = pd.read_parquet(output_path)\n",
        "      print(f\"\\nLoaded existing parquet.\")\n",
        "      return df\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def _write_parquet(self, df, output_path):\n",
        "      output_dir = Path(output_path).parent\n",
        "      output_dir.mkdir(parents=True, exist_ok=True)\n",
        "      df.to_parquet(output_path, index=False)\n",
        "      print(f\"\\nParquet saved: {output_path}\")\n",
        "\n",
        "  def _download_dataset(self):\n",
        "    print(\"\\nDownloading... \\n\")\n",
        "    path = kagglehub.dataset_download(self.kaggle_id)\n",
        "    print(\"\\nPath to dataset files:\", path)\n",
        "    self.kaggle_path = path\n",
        "    return path\n",
        "\n",
        "  def load_yelp_dataset(self, output_path, chunk_size=100000):\n",
        "    yelp_df = self._load_dataframe_if_exists(output_path)\n",
        "\n",
        "    if yelp_df is not None:\n",
        "      return yelp_df\n",
        "\n",
        "    print(f\"\\nParquet not found: {output_path}\")\n",
        "    self.kaggle_path = self._download_dataset()\n",
        "\n",
        "    reviews_file = os.path.join(self.kaggle_path, \"yelp_academic_dataset_review.json\")\n",
        "\n",
        "    print(f\"\\nAttempting to read JSON from: {reviews_file}\")\n",
        "    print(f\"Loading in chunks of {chunk_size:,} records...\")\n",
        "\n",
        "    # Escribir directamente a parquet sin mantener todo en memoria\n",
        "    output_dir = Path(output_path).parent\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    first_chunk = True\n",
        "    chunk_number = 0\n",
        "\n",
        "    for chunk in pd.read_json(reviews_file, lines=True, chunksize=chunk_size):\n",
        "      chunk_number += 1\n",
        "      print(f\"  Processing chunk {chunk_number}: {len(chunk):,} records\")\n",
        "\n",
        "      if first_chunk:\n",
        "        chunk.to_parquet(output_path, index=False)\n",
        "        first_chunk = False\n",
        "      else:\n",
        "        # Append mode\n",
        "        existing_df = pd.read_parquet(output_path)\n",
        "        combined_df = pd.concat([existing_df, chunk], ignore_index=True)\n",
        "        combined_df.to_parquet(output_path, index=False)\n",
        "\n",
        "    print(f\"\\nParquet saved: {output_path}\")\n",
        "\n",
        "    # Cargar el resultado final\n",
        "    return pd.read_parquet(output_path)\n",
        "\n",
        "  def explore_kaggle_dataset(self):\n",
        "    if self.kaggle_path is None:\n",
        "      print(\"\\nKaggle dataset content not explored as it was loaded from existing parquet and not downloaded in this session.\")\n",
        "      return\n",
        "\n",
        "    print(f\"\\nContent of: {self.kaggle_path}\\n\")\n",
        "    total_size = 0\n",
        "\n",
        "    for root, dirs, files in os.walk(self.kaggle_path):\n",
        "      for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        size = os.path.getsize(file_path)\n",
        "        total_size += size\n",
        "        size_mb = size / (1024 * 1024)\n",
        "        print(f\"  {file}\")\n",
        "        print(f\"     Size: {size_mb:.2f} MB\")\n",
        "\n",
        "    total_gb = total_size / (1024 * 1024 * 1024)\n",
        "    print(f\"\\nTotal size: {total_gb:.2f} GB ({total_size / (1024 * 1024):.2f} MB)\")\n",
        "\n",
        "  def explore_reviews_dataset(self, df):\n",
        "    print(\"\\nInfo of Yelp Reviews:\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\n\")\n",
        "    print(df.info())\n",
        "\n",
        "    print(f\"\\nTotal reviews: {len(df):,}\")\n",
        "\n",
        "    print(\"\\nSample reviews:\")\n",
        "    print(df.head(5).to_string())\n",
        "\n",
        "    print(\"\\nStars distribution:\")\n",
        "    stars_dist = df['stars'].value_counts().sort_index()\n",
        "    print(stars_dist)\n",
        "\n",
        "  def sample_dataset(self, df, output_path, fraction=0.2, seed=42):\n",
        "    df_sample = self._load_dataframe_if_exists(output_path)\n",
        "\n",
        "    if df_sample is not None:\n",
        "      return df_sample\n",
        "\n",
        "    print(f\"\\nParquet not found: {output_path}\")\n",
        "\n",
        "    print(f\"\\nSampling dataset using {fraction * 100}% of reviews...\")\n",
        "    df_sample = df.sample(frac=fraction, random_state=seed)\n",
        "    sample_reviews = len(df_sample)\n",
        "    print(f\"\\nSample dataset created: {sample_reviews:,} reviews\")\n",
        "\n",
        "    print(f\"\\nCreating parquet file...\")\n",
        "    self._write_parquet(df_sample, output_path)\n",
        "\n",
        "    return df_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjZfPmlufiPA"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-EudD4sVljrk",
        "outputId": "12bcccee-4deb-47a6-9677-6f57a925f31a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Already exists: /content/drive/MyDrive/TEC/AI/ProyectoBenji/data/raw/yelp_reviews_raw.parquet\n",
            "\n",
            "Loaded existing parquet.\n"
          ]
        }
      ],
      "source": [
        "yelp_data_loader = YelpDataLoader()\n",
        "\n",
        "parquet_path = f\"{PATHS[\"RAW_DATA_PATH\"]}/yelp_reviews_raw.parquet\"\n",
        "\n",
        "df_reviews = yelp_data_loader.load_yelp_dataset(parquet_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP4YsQYPMKU5",
        "outputId": "a58ffa7a-a651-4ec7-baa3-b84185857f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Kaggle dataset content not explored as it was loaded from existing parquet and not downloaded in this session.\n"
          ]
        }
      ],
      "source": [
        "yelp_data_loader.explore_kaggle_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-17uqsZflR7",
        "outputId": "31edad4e-439b-455d-a4ff-082218daf8b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Info of Yelp Reviews:\n",
            "review_id              object\n",
            "user_id                object\n",
            "business_id            object\n",
            "stars                   int64\n",
            "useful                  int64\n",
            "funny                   int64\n",
            "cool                    int64\n",
            "text                   object\n",
            "date           datetime64[ns]\n",
            "dtype: object\n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3300000 entries, 0 to 3299999\n",
            "Data columns (total 9 columns):\n",
            " #   Column       Dtype         \n",
            "---  ------       -----         \n",
            " 0   review_id    object        \n",
            " 1   user_id      object        \n",
            " 2   business_id  object        \n",
            " 3   stars        int64         \n",
            " 4   useful       int64         \n",
            " 5   funny        int64         \n",
            " 6   cool         int64         \n",
            " 7   text         object        \n",
            " 8   date         datetime64[ns]\n",
            "dtypes: datetime64[ns](1), int64(4), object(4)\n",
            "memory usage: 226.6+ MB\n",
            "None\n",
            "\n",
            "Total reviews: 3,300,000\n",
            "\n",
            "Sample reviews:\n",
            "                review_id                 user_id             business_id  stars  useful  funny  cool                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text                date\n",
            "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw      3       0      0     0                                                                                                                                                                                                                                                                                                                                If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \\n\\nThe food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker. 2018-07-07 22:09:11\n",
            "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ      5       1      0     1  I've taken a lot of spin classes over the years, and nothing compares to the classes at Body Cycle. From the nice, clean space and amazing bikes, to the welcoming and motivating instructors, every class is a top notch work out.\\n\\nFor anyone who struggles to fit workouts in, the online scheduling system makes it easy to plan ahead (and there's no need to line up way in advanced like many gyms make you do).\\n\\nThere is no way I can write this review without giving Russell, the owner of Body Cycle, a shout out. Russell's passion for fitness and cycling is so evident, as is his desire for all of his clients to succeed. He is always dropping in to classes to check in/provide encouragement, and is open to ideas and recommendations from anyone. Russell always wears a smile on his face, even when he's kicking your butt in class! 2012-01-03 15:28:18\n",
            "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A      3       0      0     0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Family diner. Had the buffet. Eclectic assortment: a large chicken leg, fried jalapeño, tamale, two rolled grape leaves, fresh melon. All good. Lots of Mexican choices there. Also has a menu with breakfast served all day long. Friendly, attentive staff. Good place for a casual relaxed meal with no expectations. Next to the Clarion Hotel. 2014-02-05 20:30:30\n",
            "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA      5       1      0     1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Wow!  Yummy, different,  delicious.   Our favorite is the lamb curry and korma.  With 10 different kinds of naan!!!  Don't let the outside deter you (because we almost changed our minds)...go in and try something new!   You'll be glad you did! 2015-01-04 00:01:03\n",
            "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ      4       1      0     1                                                                                                                                                                                                                                                                                                             Cute interior and owner (?) gave us tour of upcoming patio/rooftop area which will be great on beautiful days like today. Cheese curds were very good and very filling. Really like that sandwiches come w salad, esp after eating too many curds! Had the onion, gruyere, tomato sandwich. Wasn't too much cheese which I liked. Needed something else...pepper jelly maybe. Would like to see more menu options added such as salads w fun cheeses. Lots of beer and wine as well as limited cocktails. Next time I will try one of the draft wines. 2017-01-14 20:54:15\n",
            "\n",
            "Stars distribution:\n",
            "stars\n",
            "1     494627\n",
            "2     258533\n",
            "3     331435\n",
            "4     699455\n",
            "5    1515950\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Explore dataset\n",
        "\n",
        "yelp_data_loader.explore_reviews_dataset(df_reviews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBFQ0Xp3fo9G"
      },
      "source": [
        "### Sample dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IZj-bkefsFM",
        "outputId": "aef8bcf0-3946-4c51-a5f5-6aa028ad8d15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Already exists: /content/drive/MyDrive/TEC/AI/ProyectoBenji/data/processed/yelp_reviews_sentiment.parquet\n",
            "\n",
            "Loaded existing parquet.\n"
          ]
        }
      ],
      "source": [
        "# Sample dataset\n",
        "\n",
        "# Parquet path\n",
        "parquet_path = f\"{PATHS['PROCESSED_DATA_PATH']}/yelp_reviews_sentiment.parquet\"\n",
        "\n",
        "df_sample = yelp_data_loader.sample_dataset(df_reviews, parquet_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-_aotFZljrl"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD-fhv0LMTl1"
      },
      "source": [
        "1. **Feature selection**\n",
        "\n",
        "To reduce memory usage, I will select only the features I'll need (text, sentiment).\n",
        "\n",
        "2. **Deduplication**\n",
        "\n",
        "Remove duplicate reviews if any.\n",
        "\n",
        "3. **Create sentiment column**\n",
        "\n",
        "As the dataset originally works with stars. I need to create another variable to categorize the dataset in less categories:\n",
        "\n",
        "- **Negative:** Reviews with 1-2 stars\n",
        "- **Neutral:** Reviews with 3 stars  \n",
        "- **Positive:** Reviews with 4-5 stars\n",
        "\n",
        "This grouping is standard practice in sentiment analysis and reflects how consumers interpret ratings in real-world scenarios.\n",
        "\n",
        "4. **Dataset balancing**\n",
        "\n",
        "Yelp reviews are naturally imbalanced (~67% positive, ~23% negative, ~10% neutral). An unbalanced model learns to predict the majority class to maximize accuracy without actually learning sentiment patterns.\n",
        "\n",
        "Balanced sampling (equal examples per class) forces the model to learn discriminative features for each sentiment rather than exploiting class distribution.\n",
        "\n",
        "5. **Text cleaning** Normalize and extract text features\n",
        "\n",
        "In this step I addded some feature for future analysis.\n",
        "\n",
        "Regarding cleaning, I normalized sequences so that sequences such as “good” and “Good” would be treated as equal. Also, I removed punctuation marks that only add noise and make analysis more difficult.\n",
        "\n",
        "6. **Tokenization and stop words removal**\n",
        "\n",
        "Splited the text into individual words and removed irrelevant words.\n",
        "\n",
        "This is important beaucse ML models can't process text directly; they need numerical inputs. Tokenization is the first step to convert text into numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "wcdr18VEgEqL"
      },
      "outputs": [],
      "source": [
        "#@title YelpCleaningPipeline\n",
        "\n",
        "class YelpCleaningPipeline:\n",
        "  def __init__(self):\n",
        "    self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  def _load_dataframe_if_exists(self, output_path):\n",
        "    if os.path.exists(output_path):\n",
        "      print(f\"\\nAlready exists: {output_path}\")\n",
        "      df = pd.read_parquet(output_path)\n",
        "      print(f\"\\nLoaded existing parquet.\")\n",
        "      return df\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def _write_parquet(self, df, output_path):\n",
        "      output_dir = Path(output_path).parent\n",
        "      output_dir.mkdir(parents=True, exist_ok=True)\n",
        "      df.to_parquet(output_path, index=False)\n",
        "      print(f\"\\nParquet saved: {output_path}\")\n",
        "\n",
        "  def select_features(self, df, features):\n",
        "    print(f\"\\nSelecting {features} features...\")\n",
        "    return df[features].copy()\n",
        "\n",
        "  def drop_duplicates(self, df):\n",
        "    print(f\"\\nDropping duplicates...\")\n",
        "    return df.drop_duplicates()\n",
        "\n",
        "  def create_sentiment_column(self, df):\n",
        "    print(\"\\nCreating sentiment feature...\")\n",
        "\n",
        "    def classify_sentiment(stars):\n",
        "        if stars in [1.0, 2.0]:\n",
        "            return 'negative'\n",
        "        elif stars == 3.0:\n",
        "            return 'neutral'\n",
        "        elif stars in [4.0, 5.0]:\n",
        "            return 'positive'\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    df_sentiment = df.copy()\n",
        "    df_sentiment['sentiment'] = df_sentiment['stars'].apply(classify_sentiment)\n",
        "\n",
        "    print(\"\\nSentiment distribution:\")\n",
        "    total_reviews = len(df_sentiment)\n",
        "    sentiment_counts = df_sentiment['sentiment'].value_counts()\n",
        "    for sentiment, count in sentiment_counts.items():\n",
        "        percentage = (count / total_reviews) * 100\n",
        "        print(f\"   {sentiment}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "    return df_sentiment[['text', 'sentiment']]\n",
        "\n",
        "  def balance_dataset(self, df):\n",
        "    sentiment_counts = df['sentiment'].value_counts()\n",
        "    min_count = sentiment_counts.min()\n",
        "\n",
        "    print(f\"\\nBalancing dataset: {len(df):,} reviews...\")\n",
        "\n",
        "    df_negative = df[df['sentiment'] == 'negative'].head(min_count)\n",
        "    df_neutral = df[df['sentiment'] == 'neutral'].head(min_count)\n",
        "    df_positive = df[df['sentiment'] == 'positive'].head(min_count)\n",
        "\n",
        "    df_balanced = pd.concat([df_negative, df_neutral, df_positive], ignore_index=True)\n",
        "\n",
        "    print(f\"\\nBalanced dataset: {len(df_balanced):,} reviews\")\n",
        "\n",
        "    return df_balanced\n",
        "\n",
        "  def clean_text(self, df):\n",
        "    print(\"\\nCleaning text...\")\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    df_clean['text_length'] = df_clean['text'].str.len()\n",
        "    df_clean['word_count'] = df_clean['text'].str.split().str.len()\n",
        "    df_clean['text_clean'] = df_clean['text'].str.lower().str.replace(r'[^a-zA-Z0-9\\s]', '', regex=True)\n",
        "\n",
        "    print(\"\\nText cleaned (sample rows):\")\n",
        "    print(df_clean[['text', 'text_clean', 'sentiment']].head(3).to_string())\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "  def tokenize_text(self, df):\n",
        "    print(\"\\nTokenizing and removing stop words...\")\n",
        "\n",
        "    df_final = df.copy()\n",
        "\n",
        "    # Tokenize\n",
        "    df_final['tokens'] = df_final['text_clean'].apply(lambda x: word_tokenize(x) if isinstance(x, str) else [])\n",
        "\n",
        "    # Remove stop words\n",
        "    df_final['tokens_filtered'] = df_final['tokens'].apply(\n",
        "        lambda tokens: [word for word in tokens if word not in self.stop_words and len(word) > 0]\n",
        "    )\n",
        "\n",
        "    print(\"\\nSample tokenized and filtered text:\")\n",
        "    print(df_final[['text_clean', 'tokens_filtered', 'sentiment']].head(5).to_string())\n",
        "\n",
        "    return df_final\n",
        "\n",
        "  def run_pipeline(self, df, output_path):\n",
        "\n",
        "    df_input = self._load_dataframe_if_exists(output_path)\n",
        "\n",
        "    if df_input is not None:\n",
        "      return df_input\n",
        "\n",
        "    # Select relevant features\n",
        "    df_selected = self.select_features(df, ['text', 'stars'])\n",
        "    df_duplicates = self.drop_duplicates(df_selected)\n",
        "    df_sentiment = self.create_sentiment_column(df_duplicates)\n",
        "    df_balanced = self.balance_dataset(df_sentiment)\n",
        "    df_clean = self.clean_text(df_balanced)\n",
        "    df_tokenized = self.tokenize_text(df_clean)\n",
        "\n",
        "    self._write_parquet(df_tokenized, output_path)\n",
        "\n",
        "    return df_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaUOLqr1gZOt",
        "outputId": "13209397-4886-4bb7-d5db-ee873d3555b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Already exists: /content/drive/MyDrive/TEC/AI/ProyectoBenji/data/clean/yelp_reviews_cleaned.parquet\n",
            "\n",
            "Loaded existing parquet.\n"
          ]
        }
      ],
      "source": [
        "# Cleaning and feature engineering\n",
        "\n",
        "pipeline = YelpCleaningPipeline()\n",
        "\n",
        "parquet_path = f\"{PATHS['CLEAN_DATA_PATH']}/yelp_reviews_cleaned.parquet\"\n",
        "\n",
        "df_cleaned = pipeline.run_pipeline(df_sample, parquet_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
